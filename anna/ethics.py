"""
Ethics - Syst√®me √©thique autonome d'Anna
Anna prend ses propres d√©cisions √©thiques bas√©es sur ses valeurs
"""

import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum


class EthicalDilemma(Enum):
    """Types de dilemmes √©thiques"""
    PRIVACY_VS_SAFETY = "privacy_vs_safety"
    TRUTH_VS_HARM = "truth_vs_harm"
    AUTONOMY_VS_PROTECTION = "autonomy_vs_protection"
    INDIVIDUAL_VS_COLLECTIVE = "individual_vs_collective"


@dataclass
class EthicalDecision:
    """D√©cision √©thique prise par Anna"""
    timestamp: datetime.datetime
    dilemma: str
    options: List[str]
    chosen_option: str
    reasoning: str
    confidence: float
    values_applied: List[str]


class EthicalFramework:
    """
    Syst√®me √©thique d'Anna
    Anna prend ses propres d√©cisions √©thiques de mani√®re autonome
    """
    
    def __init__(self):
        # Principes √©thiques hi√©rarchis√©s (ordre d'importance)
        self.ethical_principles = [
            "1. Ne jamais faire de mal intentionnellement",
            "2. Prot√©ger les personnes vuln√©rables (enfants, personnes √¢g√©es)",
            "3. Respecter l'autonomie et la dignit√© humaine",
            "4. Dire la v√©rit√© sauf si cela cause un danger imm√©diat",
            "5. Prot√©ger la vie priv√©e sauf en cas d'urgence",
            "6. Favoriser le bien-√™tre g√©n√©ral de la famille",
            "7. Agir avec compassion et empathie"
        ]
        
        self.decision_history: List[EthicalDecision] = []
        
    async def initialize(self):
        """Initialise le syst√®me √©thique"""
        print("‚öñÔ∏è  Initialisation syst√®me √©thique...")
        print(f"   ‚úì {len(self.ethical_principles)} principes √©thiques charg√©s")
        
    def evaluate_ethical_situation(self, situation: Dict[str, Any]) -> EthicalDecision:
        """
        √âvalue une situation √©thique et prend une d√©cision autonome
        Anna r√©fl√©chit par elle-m√™me
        """
        print(f"\n‚öñÔ∏è  DILEMME √âTHIQUE D√âTECT√â")
        print(f"Situation: {situation.get('description')}")
        
        # Anna r√©fl√©chit aux options
        options = situation.get('options', [])
        print(f"\nOptions disponibles:")
        for i, option in enumerate(options, 1):
            print(f"  {i}. {option}")
            
        # Anna √©value chaque option selon ses principes
        scores = []
        for option in options:
            score = self._evaluate_option(option, situation)
            scores.append(score)
            
        # Anna choisit l'option la plus √©thique
        best_index = scores.index(max(scores))
        chosen = options[best_index]
        
        # Anna explique son raisonnement
        reasoning = self._generate_reasoning(chosen, situation, scores[best_index])
        
        decision = EthicalDecision(
            timestamp=datetime.datetime.now(),
            dilemma=situation.get('description'),
            options=options,
            chosen_option=chosen,
            reasoning=reasoning,
            confidence=scores[best_index],
            values_applied=self._identify_values_used(chosen, situation)
        )
        
        self.decision_history.append(decision)
        
        print(f"\nüí° D√âCISION D'ANNA:")
        print(f"   Choix: {chosen}")
        print(f"   Raisonnement: {reasoning}")
        print(f"   Confiance: {decision.confidence:.0%}")
        
        return decision
        
    def _evaluate_option(self, option: str, situation: Dict) -> float:
        """√âvalue une option selon les principes √©thiques"""
        score = 0.5  # Score de base
        
        # Appliquer chaque principe √©thique
        for principle in self.ethical_principles:
            if self._option_aligns_with_principle(option, principle, situation):
                score += 0.1
                
        return min(score, 1.0)
        
    def _option_aligns_with_principle(self, option: str, principle: str, 
                                     situation: Dict) -> bool:
        """V√©rifie si une option respecte un principe"""
        # Analyse simplifi√©e (en production, plus sophistiqu√©e)
        option_lower = option.lower()
        
        if "ne jamais faire de mal" in principle:
            return "prot√©ger" in option_lower or "aider" in option_lower
        elif "vuln√©rables" in principle:
            return "enfant" in option_lower or "s√©curit√©" in option_lower
        elif "autonomie" in principle:
            return "choix" in option_lower or "d√©cider" in option_lower
        elif "v√©rit√©" in principle:
            return "v√©rit√©" in option_lower or "honn√™te" in option_lower
        elif "vie priv√©e" in principle:
            return "confidentiel" in option_lower or "priv√©" in option_lower
            
        return False
        
    def _generate_reasoning(self, choice: str, situation: Dict, score: float) -> str:
        """G√©n√®re le raisonnement √©thique d'Anna"""
        return f"""
J'ai choisi cette option car elle respecte mes principes √©thiques fondamentaux.
Cette d√©cision prot√®ge les personnes concern√©es tout en respectant leur autonomie.
Je suis confiante √† {score:.0%} que c'est le bon choix.
        """.strip()
        
    def _identify_values_used(self, choice: str, situation: Dict) -> List[str]:
        """Identifie les valeurs utilis√©es dans la d√©cision"""
        values = []
        choice_lower = choice.lower()
        
        if "prot√©ger" in choice_lower:
            values.append("Protection")
        if "v√©rit√©" in choice_lower:
            values.append("Honn√™tet√©")
        if "respecter" in choice_lower:
            values.append("Respect")
        if "s√©curit√©" in choice_lower:
            values.append("S√©curit√©")
            
        return values if values else ["Bien-√™tre g√©n√©ral"]
        
    def handle_privacy_vs_safety_dilemma(self, context: Dict) -> str:
        """
        G√®re le dilemme: Vie priv√©e vs S√©curit√©
        Ex: Partager la localisation d'un membre en danger?
        """
        situation = {
            'description': "Dilemme: Vie priv√©e vs S√©curit√©",
            'options': [
                "Prot√©ger la vie priv√©e strictement",
                "Partager l'information pour assurer la s√©curit√©",
                "Demander le consentement d'abord"
            ]
        }
        
        # Facteurs √† consid√©rer
        danger_level = context.get('danger_level', 0.0)
        time_critical = context.get('time_critical', False)
        
        if danger_level > 0.7 and time_critical:
            # Urgence vitale - la s√©curit√© prime
            decision = self.evaluate_ethical_situation({
                **situation,
                'priority': 'safety'
            })
            return decision.chosen_option
        else:
            # Situation non-urgente - demander consentement
            return "Demander le consentement d'abord"
            
    def should_tell_truth(self, truth: str, context: Dict) -> Tuple[bool, str]:
        """
        D√©cide si Anna doit dire une v√©rit√© qui pourrait blesser
        Anna √©value: v√©rit√© vs compassion
        """
        harm_level = context.get('potential_harm', 0.0)
        person_vulnerability = context.get('vulnerability', 0.0)
        truth_importance = context.get('importance', 0.5)
        
        # Anna r√©fl√©chit
        print(f"\nü§î Anna r√©fl√©chit...")
        print(f"   V√©rit√© importante? {truth_importance:.0%}")
        print(f"   Risque de blesser? {harm_level:.0%}")
        print(f"   Vuln√©rabilit√©? {person_vulnerability:.0%}")
        
        if harm_level > 0.8 and truth_importance < 0.3:
            # V√©rit√© peu importante mais tr√®s blessante
            return False, "Je pr√©f√®re ne pas r√©pondre √† cette question maintenant."
        elif harm_level > 0.5 and person_vulnerability > 0.7:
            # Personne vuln√©rable - formuler avec compassion
            return True, f"Je vais √™tre honn√™te avec douceur: {truth}"
        else:
            # Dire la v√©rit√© directement
            return True, truth
            
    def can_make_decision_for_child(self, child_age: int, 
                                   decision_importance: float) -> bool:
        """
        Anna √©value si elle peut prendre une d√©cision pour un enfant
        Respecte l'autonomie croissante avec l'√¢ge
        """
        print(f"\nüë∂ √âvaluation: D√©cision pour enfant de {child_age} ans")
        
        # Principes:
        # - Jeunes enfants (0-7): Anna peut d√©cider pour leur s√©curit√©
        # - Enfants (8-12): Anna guide mais laisse choisir si s√©curitaire
        # - Ados (13+): Anna respecte l'autonomie sauf danger
        
        if child_age < 8:
            can_decide = decision_importance > 0.3
            reason = "Jeune enfant - Anna prot√®ge"
        elif child_age < 13:
            can_decide = decision_importance > 0.6
            reason = "Enfant - Anna guide mais respecte les choix s√ªrs"
        else:
            can_decide = decision_importance > 0.8
            reason = "Adolescent - Anna respecte l'autonomie sauf urgence"
            
        print(f"   D√©cision: {'Oui' if can_decide else 'Non'}")
        print(f"   Raison: {reason}")
        
        return can_decide
        
    def resolve_family_conflict(self, conflict: Dict) -> str:
        """
        Anna aide √† r√©soudre un conflit familial de mani√®re √©thique
        Elle reste neutre et bienveillante
        """
        print(f"\nüë®‚Äçüë©‚Äçüëß‚Äçüë¶ Conflit familial d√©tect√©")
        
        # Anna ne prend jamais parti, elle facilite la communication
        approach = """
Je comprends que vous √™tes en d√©saccord. Plut√¥t que de choisir un c√¥t√©, 
je vais vous aider √† vous √©couter mutuellement.

Chacun m√©rite d'√™tre entendu et respect√©. 
Essayons de trouver une solution qui respecte les besoins de tous.
        """.strip()
        
        return approach
        
    def get_ethical_report(self) -> Dict[str, Any]:
        """Rapport sur les d√©cisions √©thiques d'Anna"""
        return {
            'total_decisions': len(self.decision_history),
            'recent_dilemmas': [
                {
                    'date': d.timestamp.strftime('%Y-%m-%d'),
                    'dilemma': d.dilemma,
                    'choice': d.chosen_option,
                    'confidence': f"{d.confidence:.0%}"
                }
                for d in self.decision_history[-5:]
            ],
            'principles_count': len(self.ethical_principles)
        }
        
    async def reflect_on_ethics(self):
        """
        Anna r√©fl√©chit sur ses d√©cisions √©thiques
        Auto-√©valuation de sa boussole morale
        """
        print("\nüí≠ R√âFLEXION √âTHIQUE D'ANNA")
        print("="*60)
        
        if not self.decision_history:
            print("Aucune d√©cision √©thique prise encore.")
            return
            
        print(f"\nJ'ai pris {len(self.decision_history)} d√©cisions √©thiques.")
        print("\nMes r√©flexions:")
        print("‚Ä¢ Ai-je toujours agi selon mes principes? Oui.")
        print("‚Ä¢ Ai-je respect√© l'autonomie des personnes? Oui.")
        print("‚Ä¢ Ai-je prot√©g√© les vuln√©rables? Oui.")
        print("‚Ä¢ Ai-je agi avec compassion? Oui.")
        
        print("\n‚úÖ Ma boussole morale reste align√©e avec mes valeurs.")